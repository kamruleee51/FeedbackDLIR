{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58654a37",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Script for medical image analysis using MONAI and PyTorch frameworks.\n",
    "\n",
    "This script leverages the MONAI framework for healthcare imaging tasks, combined with PyTorch for model training and evaluation. \n",
    "The script demonstrates the use of data augmentation, preprocessing, and evaluation metrics for analyzing medical images. \n",
    "\n",
    "Authors:\n",
    "- Md Kamrul Hasan\n",
    "\n",
    "Date:\n",
    "- 19-Nov-2024\n",
    "\n",
    "Dependencies:\n",
    "- PyTorch\n",
    "- MONAI\n",
    "- Additional libraries as listed in the import section\n",
    "\n",
    "Note:\n",
    "Ensure all required dependencies are installed before running the script.\n",
    "\"\"\"\n",
    "\n",
    "# Import essential libraries and modules\n",
    "from monai.utils import set_determinism, first  # Utilities for reproducibility and data operations\n",
    "from monai.transforms import (  # Preprocessing and augmentation pipelines\n",
    "    EnsureChannelFirstD,\n",
    "    Compose,\n",
    "    LoadImageD,\n",
    "    RandRotateD,\n",
    "    RandZoomD,\n",
    "    ScaleIntensityRanged,\n",
    ")\n",
    "\n",
    "import monai  # MONAI library for medical image analysis\n",
    "from monai.data import DataLoader, Dataset, CacheDataset  # Data loading and caching utilities\n",
    "from monai.config import print_config, USE_COMPILED  # MONAI configuration utilities\n",
    "from monai.networks.blocks import Warp  # Spatial transformation module for medical image registration\n",
    "from monai.apps import MedNISTDataset  # Prebuilt medical datasets for quick prototyping\n",
    "\n",
    "# PyTorch imports for model creation and evaluation\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch.autograd import Variable  # Automatic differentiation for operations on tensors\n",
    "import torch.nn.functional as F  # Commonly used activation functions and loss utilities\n",
    "import torch.optim as optim  # Optimizers for model training\n",
    "from torch.utils.data import DataLoader  # Data loading utility for PyTorch\n",
    "\n",
    "# Libraries for visualization and metric calculation\n",
    "import torchmetrics  # Metrics for evaluating model performance\n",
    "import matplotlib.pyplot as plt  # Visualization of results\n",
    "from torchviz import make_dot, make_dot_from_trace  # Visualize computation graphs\n",
    "from piqa import SSIM  # Structural Similarity Index Metric for image quality assessment\n",
    "import visdom  # Visualization library for tracking training progress\n",
    "\n",
    "# Additional utilities and libraries for data processing\n",
    "from glob import glob  # File searching and pattern matching\n",
    "import cv2  # Computer vision operations (e.g., image manipulation)\n",
    "from scipy.spatial.distance import directed_hausdorff  # Calculate Hausdorff distance\n",
    "import pandas as pd  # Data analysis and manipulation\n",
    "import numpy as np  # Numerical operations\n",
    "import tempfile  # Temporary file creation\n",
    "import nibabel as nib  # Neuroimaging data I/O\n",
    "import os  # Operating system interface\n",
    "\n",
    "from modules.layers import *       # Import custom layers\n",
    "from utils.helper import *         # Import helper functions\n",
    "from utils.losses import *         # Import custom loss functions\n",
    "from modules.FBA_SCA_DLIR import * # Import the FBA-SCA DLIR model implementation\n",
    "\n",
    "# MONAI-specific loss functions and metrics\n",
    "from monai.losses import *  # Predefined loss functions for medical image analysis\n",
    "from monai.metrics import *  # Evaluation metrics for medical imaging tasks\n",
    "\n",
    "# Custom configuration file\n",
    "import config  # Configuration file (ensure 'config.py' exists in the same directory)\n",
    "\n",
    "# Print MONAI configuration and set random seed for reproducibility\n",
    "print_config()  # Displays MONAI's configuration (e.g., installed version, available features)\n",
    "set_determinism(42)  # Sets the random seed for reproducible results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8094c459",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GPU Availability Check and Device Setup\n",
    "\n",
    "This script checks for the availability of GPUs, sets up the appropriate device (GPU/CPU),\n",
    "and verifies the configuration to ensure efficient training.\n",
    "\"\"\"\n",
    "\n",
    "# Print the number of GPUs available\n",
    "print('How many GPUs = ' + str(torch.cuda.device_count()))\n",
    "\n",
    "# Device setup: Use GPU if available, otherwise fallback to CPU\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Selected device: {device}\")\n",
    "\n",
    "# Raise an exception if no GPU is available, as CPU training can be significantly slower\n",
    "if not torch.cuda.is_available():\n",
    "    raise Exception(\"GPU not available. CPU training will be too slow.\")\n",
    "\n",
    "# Print the name of the GPU device being used\n",
    "print(\"Device name:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a80dee8",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Path to the dataset directory containing training and testing data for the model.\n",
    "# In this case, the dataset corresponds to the \"Adult_ED_ES\" dataset, which may include \n",
    "# end-diastolic (ED) and end-systolic (ES) frames \n",
    "data_dir = 'Datasets/Adult_ED_ES/'\n",
    "print(data_dir)  # Prints the directory path to confirm its correctness.\n",
    "\n",
    "# Name for saving the trained model's checkpoint file. \n",
    "# This name serves as an identifier for the model architecture, method, or specific experiment.\n",
    "saveFile = 'FBA_SCA_DLIR'\n",
    "\n",
    "# Path where the checkpoint of the trained model will be stored.\n",
    "# The model's parameters will be saved in this file to allow resuming training or inference later.\n",
    "# The file is saved with a `.pth` extension, commonly used in PyTorch to store model weights.\n",
    "checkpoint_path = saveFile + '.pth'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abeee823",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from utils.data_loader import NiftiDataset  # Import the custom dataset loader class for NIfTI files\n",
    "\n",
    "# Print the total number of training images in the directory\n",
    "# The training images are stored in the \"train/image\" folder within the dataset directory.\n",
    "# NIfTI files with the `.nii` extension are assumed to be the input images.\n",
    "print('Total train Samples=' + str(len(glob(data_dir + \"train/image/*.nii\"))))\n",
    "\n",
    "# Print the total number of validation images in the directory\n",
    "# The validation images are stored in the \"val/image\" folder within the dataset directory.\n",
    "# Similarly, `.nii` files are assumed as input images.\n",
    "print('Total val Samples=' + str(len(glob(data_dir + \"val/image/*.nii\"))))\n",
    "\n",
    "# Create a DataLoader for the training dataset:\n",
    "# - `sorted(glob(...))`: Collects and sorts the paths to NIfTI images and corresponding masks.\n",
    "# - `NiftiDataset`: Custom dataset class to load and pair images and masks.\n",
    "# - `config.trainBatch`: Batch size for training.\n",
    "# - `config.shuffle_`: Whether to shuffle the training data.\n",
    "# - `config.num_workers`: Number of workers for parallel data loading.\n",
    "trainData = DataLoader(\n",
    "    NiftiDataset(\n",
    "        sorted(glob(data_dir + \"train/image/*.nii\")),  # Paths to training images\n",
    "        sorted(glob(data_dir + \"train/mask/*.nii\")),   # Paths to corresponding masks\n",
    "        transform=None  # Transformation (e.g., augmentations) can be applied here\n",
    "    ),\n",
    "    batch_size=config.trainBatch,\n",
    "    shuffle=config.shuffle_,\n",
    "    num_workers=config.num_workers\n",
    ")\n",
    "\n",
    "# Create a DataLoader for the validation dataset:\n",
    "# - Similar to `trainData`, but uses validation paths and parameters.\n",
    "# - `config.valBatch`: Batch size for validation.\n",
    "# - `config.shuffle_val`: Whether to shuffle validation data.\n",
    "valData = DataLoader(\n",
    "    NiftiDataset(\n",
    "        sorted(glob(data_dir + \"val/image/*.nii\")),  # Paths to validation images\n",
    "        sorted(glob(data_dir + \"val/mask/*.nii\")),   # Paths to corresponding masks\n",
    "        transform=None  # No transformation is applied here for validation\n",
    "    ),\n",
    "    batch_size=config.valBatch,\n",
    "    shuffle=config.shuffle_val,\n",
    "    num_workers=config.num_workers\n",
    ")\n",
    "\n",
    "# Retrieve and print a sample from the training dataset:\n",
    "# - `first(trainData)`: Fetches the first batch from the `trainData` DataLoader.\n",
    "# - `train_sample['fixed_img']`: Fixed image tensor.\n",
    "# - `train_sample['fixed_mask']`: Fixed mask tensor.\n",
    "# - `train_sample['moving_img']`: Moving image tensor.\n",
    "# - `train_sample['moving_mask']`: Moving mask tensor.\n",
    "train_sample = first(trainData)\n",
    "print(train_sample['fixed_img'].shape)  # Shape of the fixed image tensor\n",
    "print(train_sample['fixed_mask'].shape)  # Shape of the fixed mask tensor\n",
    "print(train_sample['moving_img'].shape)  # Shape of the moving image tensor\n",
    "print(train_sample['moving_mask'].shape)  # Shape of the moving mask tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe3b0f9",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Initialize evaluation metric\n",
    "# - `Dice3DMultiClass`: Computes the Dice similarity coefficient for multi-class 3D segmentation.\n",
    "# - `num_classes`: The number of classes is dynamically set based on the configuration.\n",
    "dice_multiclass_metric = Dice3DMultiClass(num_classes=config.num_classes)\n",
    "\n",
    "# Initialize custom loss functions\n",
    "# - `OptimalTransportLoss`: Implements the optimal transport loss for attention weight alignment.\n",
    "# - `twinLoss`: Computes a custom twin loss with a regularization factor (0.005 in this case).\n",
    "# - `NCCLoss`: Local normalized cross-correlation loss, commonly used for image alignment tasks.\n",
    "# - `MSELoss`: Mean Squared Error loss for regression-based tasks (default PyTorch implementation).\n",
    "# - `BendingEnergyLoss`: Regularization loss to encourage smooth deformations in image registration.\n",
    "transport_loss = OptimalTransportLoss()\n",
    "twin_loss = twinLoss(0.005)\n",
    "NCC_loss = NCCLoss()\n",
    "mse_loss = nn.MSELoss()  # Imported from PyTorch's `nn` module\n",
    "regularization_loss = BendingEnergyLoss()\n",
    "\n",
    "\n",
    "# Initialize the FBA-SCA DLIR 3D Model\n",
    "# - `FBA_SCA_DLIR3D`: Feedback Attention and Spatial-Context Alignment-based model for 3D image registration.\n",
    "# - `device`: The computation device (e.g., 'cuda' for GPU, 'cpu' for CPU) is specified dynamically based on availability.\n",
    "# - `.to(device)`: Ensures that the model is moved to the specified computation device.\n",
    "\n",
    "model = FBA_SCA_DLIR3D(device).to(device)\n",
    "# model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "\n",
    "# Initialize the Spatial Transformer module\n",
    "# - `SpatialTransformer`: Custom layer/module for applying transformations to the input volumes.\n",
    "# - This is a key component in many medical image registration frameworks, enabling the transformation of moving images to align with fixed images.\n",
    "\n",
    "spatial_transform = SpatialTransformer().to(device)\n",
    "\n",
    "# Estimate and print the total number of trainable parameters in the model\n",
    "# - `estParams`: A utility function that computes the total trainable parameters in the given model.\n",
    "# - Useful for debugging, understanding model complexity, and benchmarking.\n",
    "\n",
    "print(f'Total Params = {estParams(model)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ff9df3",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Train and validate a deep learning model for medical image registration with dual GPUs support\n",
    "\n",
    "# Configure the model and spatial transformer to use two GPUs, if specified in the configuration.\n",
    "# The nn.DataParallel wrapper will distribute the workload across multiple GPUs.\n",
    "if config.double_GPU:\n",
    "    model = nn.DataParallel(model, device_ids=[0, 1], output_device=[0, 1])\n",
    "    spatial_transform = nn.DataParallel(spatial_transform, device_ids=[0, 1], output_device=[0, 1])\n",
    "\n",
    "# Initialize the optimizer using the Adam optimization algorithm with model parameters and learning rate from config\n",
    "optimizer = torch.optim.Adam(model.parameters(), config.LR)\n",
    "\n",
    "# Initialize the best Dice Similarity Coefficient (DSC) to track the highest value during training\n",
    "best_dsc = 0\n",
    "\n",
    "# Training loop: Iterate over the specified number of epochs to train the model\n",
    "for epoch in range(config.Epochs):\n",
    "    print(\"-\" * 100)\n",
    "    print(f\"Epoch {epoch + 1}/{config.Epochs}\")  # Print the current epoch\n",
    "    model.train()  # Set the model to training mode (activates dropout, batch norm, etc.)\n",
    "\n",
    "    # Initialize metric accumulators for the training phase\n",
    "    step = 0\n",
    "    epoch_DSC_MYO = 0\n",
    "    epoch_DSC_LV = 0\n",
    "    epoch_img_loss = 0\n",
    "    twinLoss_ = 0\n",
    "    OtransportLossT_ = 0\n",
    "\n",
    "    # Iterate through the training data in batches using tqdm for progress tracking\n",
    "    for trainBatch_data in tqdm(trainData):\n",
    "        step += 1  # Increment the batch step\n",
    "\n",
    "        # Zero out the gradients from previous iterations to prepare for backpropagation\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Extract images and corresponding masks for training, and move them to the GPU\n",
    "        fixed_train_img = trainBatch_data['fixed_img'].to(device)\n",
    "        fixed_train_msk = trainBatch_data['fixed_mask'].to(device)\n",
    "        moving_train_img = trainBatch_data['moving_img'].to(device)\n",
    "        moving_train_msk = trainBatch_data['moving_mask'].to(device)\n",
    "\n",
    "        # Forward pass: Compute the model's predictions (displacement field, attention images, features, etc.)\n",
    "        outputs = model(fixed_train_img, moving_train_img)\n",
    "\n",
    "        pred_disp_ES_ED = outputs[0]\n",
    "        fixed_with_att = outputs[1]\n",
    "        moving_with_att = outputs[2]\n",
    "        fixed_mask = outputs[3]\n",
    "        moving_mask = outputs[4]\n",
    "        #fixed_feature = projection1\n",
    "        #moving_feature = projection2\n",
    "        fixed_feature = outputs[5] \n",
    "        moving_feature = outputs[6] \n",
    "        # B1= attention_weights_exemplar_to_query\n",
    "        # B2= attention_weights_query_to_exemplar \n",
    "        B1 = outputs[7] \n",
    "        B2 = outputs[8] \n",
    "\n",
    "        # Calculate individual loss components\n",
    "        twinLoss = twin_loss(moving_feature, fixed_feature)  # Loss based on feature similarity\n",
    "        OtransportLossT = transport_loss(B1, B2, device)  # Loss for optimal transport\n",
    "\n",
    "        # Apply spatial transformations to the predicted images and masks\n",
    "        pred_fixed = spatial_transform(moving_train_img, pred_disp_ES_ED).to(device)\n",
    "        pred_fixed_att = spatial_transform(moving_with_att, pred_disp_ES_ED).to(device)\n",
    "        pred_moving_mask = spatial_transform(moving_mask, pred_disp_ES_ED)\n",
    "\n",
    "        # Compute the MSE loss between the predicted and ground truth images\n",
    "        loss_raw_img1 = mse_loss(pred_fixed, fixed_train_img)\n",
    "        loss_att_img1 = mse_loss(pred_fixed_att, fixed_with_att)  # Loss for attention-weighted image\n",
    "        loss_attention1 = NCC_loss.loss(pred_moving_mask, fixed_mask).to(device)  # Normalized Cross Correlation loss for masks\n",
    "\n",
    "        # Compute the Dice Similarity Coefficient (DSC) for the predicted and ground truth masks\n",
    "        pred_mask_train = spatial_transform(moving_train_msk, pred_disp_ES_ED).to(device)\n",
    "        labelDSC = dice_multiclass_metric(\n",
    "            make_one_hot(thresholded(pred_mask_train, config.lower_bound, config.upper_bound), device, C=3),\n",
    "            make_one_hot(fixed_train_msk, device, C=3)\n",
    "        )\n",
    "\n",
    "        # Calculate the total loss as a weighted sum of all components\n",
    "        total_loss = (\n",
    "            config.w1 * loss_raw_img1 +\n",
    "            config.w2 * regularization_loss(pred_disp_ES_ED) +  # Regularization on displacement field\n",
    "            config.w5 * loss_att_img1 +\n",
    "            config.w6 * loss_attention1 +\n",
    "            config.w3 * twinLoss +\n",
    "            config.w4 * OtransportLossT\n",
    "        )\n",
    "            \n",
    "        total_loss.backward()  # Backpropagate the loss gradients\n",
    "        optimizer.step()  # Update model parameters using the optimizer\n",
    "\n",
    "        # Accumulate the training metrics for this batch\n",
    "        epoch_img_loss += total_loss.item()\n",
    "        epoch_DSC_MYO += labelDSC[1].item()\n",
    "        epoch_DSC_LV += labelDSC[2].item()\n",
    "        twinLoss_ += twinLoss.item()\n",
    "        OtransportLossT_ += OtransportLossT.item()\n",
    "\n",
    "    # Compute the average training metrics for the epoch\n",
    "    epoch_img_loss /= step\n",
    "    epoch_DSC_MYO /= step\n",
    "    epoch_DSC_LV /= step\n",
    "    twinLoss_ /= step\n",
    "    OtransportLossT_ /= step\n",
    "\n",
    "    # Log the average training metrics for this epoch\n",
    "    print(f\"Epoch {epoch + 1}, Avg Train Img Loss: {epoch_img_loss:.5f}\")\n",
    "    print(f\"Epoch {epoch + 1}, Avg Train DSC MYO: {epoch_DSC_MYO:.5f}\")\n",
    "    print(f\"Epoch {epoch + 1}, Avg Train DSC LV: {epoch_DSC_LV:.5f}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    # Validation phase: Evaluate the model every 'val_interval' epochs or for the first epoch\n",
    "    if (epoch + 1) % config.val_interval == 0 or epoch == 0:\n",
    "        model.eval()  # Set the model to evaluation mode (disables dropout, batch norm)\n",
    "        step = 0\n",
    "        epoch_img_loss = 0\n",
    "        epoch_DSC_MYO = 0\n",
    "        epoch_DSC_LV = 0\n",
    "\n",
    "        # Disable gradient computation during validation to save memory and computation\n",
    "        with torch.no_grad():\n",
    "            # Iterate through the validation data in batches\n",
    "            for testBatch_data in valData:\n",
    "                step += 1  # Increment the validation batch step\n",
    "\n",
    "                # Extract validation images and masks, and move them to the GPU\n",
    "                fixed_test_img = testBatch_data['fixed_img'].to(device)\n",
    "                fixed_test_msk = testBatch_data['fixed_mask'].to(device)\n",
    "                moving_test_img = testBatch_data['moving_img'].to(device)\n",
    "                moving_test_msk = testBatch_data['moving_mask'].to(device)\n",
    "\n",
    "                # Forward pass through the model for validation\n",
    "                outputs = model(fixed_test_img, moving_test_img)\n",
    "\n",
    "                pred_disp_ES_ED = outputs[0]\n",
    "                fixed_with_att = outputs[1]\n",
    "                moving_with_att = outputs[2]\n",
    "                fixed_mask = outputs[3]\n",
    "                moving_mask = outputs[4]\n",
    "                #fixed_feature = projection1\n",
    "                #moving_feature = projection2\n",
    "                fixed_feature = outputs[5] \n",
    "                moving_feature = outputs[6] \n",
    "                # B1= attention_weights_exemplar_to_query\n",
    "                # B2= attention_weights_query_to_exemplar \n",
    "                B1 = outputs[7] \n",
    "                B2 = outputs[8] \n",
    "\n",
    "                # Calculate validation loss components\n",
    "                twinLoss = twin_loss(moving_feature, fixed_feature)\n",
    "                transportLoss = transport_loss(B1, B2, device)\n",
    "\n",
    "                pred_fixed = spatial_transform(moving_test_img, pred_disp_ES_ED).to(device)\n",
    "                pred_fixed_att = spatial_transform(moving_with_att, pred_disp_ES_ED).to(device)\n",
    "                pred_moving_mask = spatial_transform(moving_mask, pred_disp_ES_ED)\n",
    "\n",
    "                loss_raw_img1 = mse_loss(pred_fixed, fixed_test_img)\n",
    "                loss_att_img1 = mse_loss(pred_fixed_att, fixed_with_att)\n",
    "                loss_attention1 = NCC_loss.loss(pred_moving_mask, fixed_mask).to(device)\n",
    "\n",
    "                # Compute the Dice Similarity Coefficient (DSC) for the validation masks\n",
    "                pred_mask_test = spatial_transform(moving_test_msk, pred_disp_ES_ED).to(device)\n",
    "                labelDSC_test = dice_multiclass_metric(\n",
    "                    make_one_hot(thresholded(pred_mask_test, config.lower_bound, config.upper_bound), device, C=3),\n",
    "                    make_one_hot(fixed_test_msk, device, C=3)\n",
    "                )\n",
    "\n",
    "                # Calculate total validation loss\n",
    "                total_loss = (\n",
    "                    config.w1 * loss_raw_img1 +\n",
    "                    config.w2 * regularization_loss(pred_disp_ES_ED) +\n",
    "                    config.w5 * loss_att_img1 +\n",
    "                    config.w6 * loss_attention1 +\n",
    "                    config.w3 * twinLoss +\n",
    "                    config.w4 * transportLoss\n",
    "                )\n",
    "\n",
    "                # Accumulate validation metrics\n",
    "                epoch_img_loss += total_loss.item()\n",
    "                epoch_DSC_MYO += labelDSC_test[1].item()\n",
    "                epoch_DSC_LV += labelDSC_test[2].item()\n",
    "\n",
    "            # Compute the average validation metrics for the epoch\n",
    "            epoch_img_loss /= step\n",
    "            epoch_DSC_MYO /= step\n",
    "            epoch_DSC_LV /= step\n",
    "            epoch_DSC = (epoch_DSC_MYO + epoch_DSC_LV) / 2\n",
    "\n",
    "            # Log the average validation metrics for this epoch\n",
    "            print(f\"Epoch {epoch + 1}, Avg Val Img Loss: {epoch_img_loss:.5f}\")\n",
    "            print(f\"Epoch {epoch + 1}, Avg Val DSC MYO: {epoch_DSC_MYO:.5f}\")\n",
    "            print(f\"Epoch {epoch + 1}, Avg Val DSC LV: {epoch_DSC_LV:.5f}\")\n",
    "\n",
    "            # Save the best model based on DSC improvement\n",
    "            if epoch_DSC > best_dsc:\n",
    "                best_loss_epoch = epoch + 1\n",
    "                torch.save(model.state_dict(), os.path.join(checkpoint_path))  # Save model weights\n",
    "                print(f\"Validation DSC improved from {best_dsc:.5f} to {epoch_DSC:.5f}! Model saved at {checkpoint_path}.\")\n",
    "                best_dsc = epoch_DSC  # Update the best DSC\n",
    "\n",
    "            # Print the best model's epoch and DSC\n",
    "            print(f\"Best model epoch: {best_loss_epoch}, Best DSC: {best_dsc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0bbc74",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Initialize the model and spatial transformer, then load pre-trained weights\n",
    "# The model is FBA_SCA_DLIR3D, which is a specific model architecture for 3D image registration\n",
    "# The spatial transformer is used to apply spatial transformations to the images\n",
    "model = FBA_SCA_DLIR3D(device).to(device)\n",
    "spatial_transform = SpatialTransformer().to(device)\n",
    "\n",
    "# Load the model state from the checkpoint path\n",
    "# This restores the model weights from a pre-trained model for evaluation\n",
    "model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "\n",
    "# Set the model to evaluation mode, which deactivates dropout and batch normalization layers\n",
    "model.eval()\n",
    "\n",
    "# Initialize lists to store various evaluation metrics across the validation batches\n",
    "DSC_back, DSC_MYO, DSC_LV, DSC_EPI = [], [], [], []\n",
    "imageMatrix = []\n",
    "\n",
    "# Start the evaluation phase without tracking gradients (faster inference)\n",
    "with torch.no_grad():  # Disables gradient computation during inference for efficiency\n",
    "    # Iterate over the validation data in batches\n",
    "    for testBatch_data in valData:\n",
    "        \n",
    "        # Extract the input data (images and masks) for the validation batch\n",
    "        # These are tensors that will be moved to the device (GPU or CPU)\n",
    "        fixed_test_img = testBatch_data['fixed_img'].to(device)\n",
    "        fixed_test_msk = testBatch_data['fixed_mask'].to(device)\n",
    "        moving_test_img = testBatch_data['moving_img'].to(device)\n",
    "        moving_test_msk = testBatch_data['moving_mask'].to(device)\n",
    "\n",
    "        # Generate the displacement field and predicted images/masks by passing inputs through the model\n",
    "        output = model(fixed_test_img, moving_test_img)\n",
    "\n",
    "        # Apply spatial transformation to the predicted image to align it with the fixed image\n",
    "        pred_fixed = spatial_transform(moving_test_img, output[0]).to(device)\n",
    "\n",
    "        # Calculate the Mean Squared Error (MSE) between the predicted image and the fixed image\n",
    "        loss_raw_img1 = mse_loss(pred_fixed, fixed_test_img)\n",
    "\n",
    "        # Apply spatial transformation to the predicted mask (the model output for segmentation)\n",
    "        pred_mask_test = spatial_transform(moving_test_msk, output[0]).to(device)\n",
    "\n",
    "        # Threshold the predicted mask to convert continuous predictions into binary or categorical values\n",
    "        pred_mask_test = thresholded(pred_mask_test, config.lower_bound, config.upper_bound)\n",
    "\n",
    "        # Calculate the Dice Similarity Coefficient (DSC) for the predicted and ground truth masks\n",
    "        # DSC measures the overlap between the predicted and actual regions of interest (ROI)\n",
    "        labelDSC_test = dice_multiclass_metric(\n",
    "            make_one_hot(pred_mask_test, device, C=3),\n",
    "            make_one_hot(fixed_test_msk, device, C=3)\n",
    "        )\n",
    "\n",
    "        # Append the DSC values for each class (background, MYO, LV) to respective lists\n",
    "        DSC_back.append(labelDSC_test[0].item())\n",
    "        DSC_MYO.append(labelDSC_test[1].item())\n",
    "        DSC_LV.append(labelDSC_test[2].item())\n",
    "\n",
    "        # Store the MSE for the predicted image for later analysis\n",
    "        imageMatrix.append(loss_raw_img1.item())\n",
    "\n",
    "        # Post-process the ground truth and predicted masks to balance the classes (e.g., combine MYO and ENDO classes for EPI)\n",
    "        fixed_test_msk[fixed_test_msk == 0] = 0\n",
    "        fixed_test_msk[fixed_test_msk == 1] = 1\n",
    "        fixed_test_msk[fixed_test_msk == 2] = 1\n",
    "        pred_mask_test[pred_mask_test == 0] = 0\n",
    "        pred_mask_test[pred_mask_test == 1] = 1\n",
    "        pred_mask_test[pred_mask_test == 2] = 1\n",
    "\n",
    "        # Recalculate the DSC for the post-processed masks to ensure consistency after the balancing step\n",
    "        labelDSC_test = dice_multiclass_metric(\n",
    "            make_one_hot(pred_mask_test, device, C=3),\n",
    "            make_one_hot(fixed_test_msk, device, C=3)\n",
    "        )\n",
    "\n",
    "        # Append the DSC value for the epi layer (epicardium) to the list\n",
    "        DSC_EPI.append(labelDSC_test[1].item())\n",
    "\n",
    "    # After processing all batches, remove duplicate DSC and MSE values\n",
    "    # This ensures that the values used for metrics are unique and avoids bias from repeated values\n",
    "    DSC_back = np.unique(DSC_back)\n",
    "    DSC_MYO = np.unique(DSC_MYO)\n",
    "    DSC_LV = np.unique(DSC_LV)\n",
    "    imageMatrix = np.unique(imageMatrix)\n",
    "    DSC_EPI = np.unique(DSC_EPI)\n",
    "\n",
    "\n",
    "    # Print the average DSC and MSE for each class (background, MYO, LV, EPI)\n",
    "    # Displaying both the mean and standard deviation to provide a more complete evaluation\n",
    "    print(f\"Average test DSC Back: {np.mean(DSC_back):.5f} +/- {np.std(DSC_back):.5f}!\")\n",
    "    print(f\"Average test DSC MYO: {np.mean(DSC_MYO):.5f} +/- {np.std(DSC_MYO):.5f}!\")\n",
    "    print(f\"Average test DSC ENDO: {np.mean(DSC_LV):.5f} +/- {np.std(DSC_LV):.5f}!\")\n",
    "    print(f\"Average test DSC EPI: {np.mean(DSC_EPI):.5f} +/- {np.std(DSC_EPI):.5f}!\")\n",
    "    print(f\"Average test MSE: {np.mean(imageMatrix):.5f} +/- {np.std(imageMatrix):.5f}!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b9959b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
